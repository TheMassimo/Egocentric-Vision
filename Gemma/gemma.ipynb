{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MMAcssHTML"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQXQaW_hv5RT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/pytorch_gemma\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/pytorch_gemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXNm5_p_oxMF"
      },
      "source": [
        "# Gemma in PyTorch\n",
        "\n",
        "This is a quick demo of running Gemma inference in PyTorch.\n",
        "For more details, please check out the Github repo of the official PyTorch implementation [here](https://github.com/google/gemma_pytorch).\n",
        "\n",
        "**Note that**:\n",
        " * The free Colab CPU Python runtime and T4 GPU Python runtime are sufficient for running the Gemma 2B models and 7B int8 quantized models.\n",
        " * For advanced use cases for other GPUs or TPU, please refer to [README.md](https://github.com/google/gemma_pytorch/blob/main/README.md) in the official repo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbza6uQdA-0P"
      },
      "source": [
        "### 1. Set up Kaggle access for Gemma\n",
        "\n",
        "To complete this tutorial, you first need to follow the setup instructions at [Gemma setup](https://ai.google.dev/gemma/docs/setup), which show you how to do the following:\n",
        "\n",
        "* Get access to Gemma on [kaggle.com](https://www.kaggle.com/models/google/gemma/).\n",
        "* Select a Colab runtime with sufficient resources to run the Gemma model.\n",
        "* Generate and configure a Kaggle username and API key.\n",
        "\n",
        "After you've completed the Gemma setup, move on to the next section, where you'll set environment variables for your Colab environment.\n",
        "\n",
        "### 2. Set environment variables\n",
        "\n",
        "Set environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`. When prompted with the \"Grant access?\" messages, agree to provide secret access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0qu4_r3PycgW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"giovanniorani\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"3bc10ae36068df0511a6ef5c9c01dbf7\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqq3HDVfA6Xm"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bMboT70Xop8G"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U torch immutabledict sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdjDV3nBG5Z"
      },
      "source": [
        "## Download model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GU5ZZzcZ6ik3"
      },
      "outputs": [],
      "source": [
        "# Choose variant and machine type\n",
        "VARIANT = '9b' #@param ['2b', '2b-it', '9b', '9b-it', '27b', '27b-it']\n",
        "MACHINE_TYPE = 'cuda' #@param ['cuda', 'cpu']\n",
        "\n",
        "CONFIG = VARIANT[:2]\n",
        "if CONFIG == '2b':\n",
        "  CONFIG = '2b-v2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (0.3.6)\n",
            "Requirement already satisfied: packaging in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages (from requests->kagglehub) (2024.12.14)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ONRhkIDrE4Un"
      },
      "outputs": [
        {
          "ename": "KaggleApiHTTPError",
          "evalue": "404 Client Error.\n\nResource not found at URL: https://www.kaggle.com/models/google/gemma-2/pyTorch/gemma-2-9b\nThe server reported the following issues: Not found\nPlease make sure you specified the correct resource identifiers.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/exceptions.py:62\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.kaggle.com/api/v1/models/google/gemma-2/pyTorch/gemma-2-9b/get",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKaggleApiHTTPError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load model weights\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m weights_dir \u001b[38;5;241m=\u001b[39m \u001b[43mkagglehub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_download\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-2/pyTorch/gemma-2-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mVARIANT\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/models.py:30\u001b[0m, in \u001b[0;36mmodel_download\u001b[0;34m(handle, path, force_download)\u001b[0m\n\u001b[1;32m     28\u001b[0m h \u001b[38;5;241m=\u001b[39m parse_model_handle(handle)\n\u001b[1;32m     29\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;241m.\u001b[39mto_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, extra\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mEXTRA_CONSOLE_BLOCK})\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/registry.py:23\u001b[0m, in \u001b[0;36mMultiImplRegistry.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/http_resolver.py:151\u001b[0m, in \u001b[0;36mModelHttpResolver.__call__\u001b[0;34m(self, h, path, force_download)\u001b[0m\n\u001b[1;32m    148\u001b[0m api_client \u001b[38;5;241m=\u001b[39m KaggleApiV1Client()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m h\u001b[38;5;241m.\u001b[39mis_versioned():\n\u001b[0;32m--> 151\u001b[0m     h\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[43m_get_current_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m model_path \u001b[38;5;241m=\u001b[39m load_from_cache(h, path)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_download:\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/http_resolver.py:282\u001b[0m, in \u001b[0;36m_get_current_version\u001b[0;34m(api_client, h)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_current_version\u001b[39m(api_client: KaggleApiV1Client, h: ResourceHandle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(h, ModelHandle):\n\u001b[0;32m--> 282\u001b[0m         json_response \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_build_get_instance_url_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m MODEL_INSTANCE_VERSION_FIELD \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m json_response:\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid GetModelInstance API response. Expected to include a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_INSTANCE_VERSION_FIELD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m field\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/clients.py:117\u001b[0m, in \u001b[0;36mKaggleApiV1Client.get\u001b[0;34m(self, path, resource_handle)\u001b[0m\n\u001b[1;32m    110\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_url(path)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    112\u001b[0m     url,\n\u001b[1;32m    113\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_user_agent()},\n\u001b[1;32m    114\u001b[0m     auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_auth(),\n\u001b[1;32m    115\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m(DEFAULT_CONNECT_TIMEOUT, DEFAULT_READ_TIMEOUT),\n\u001b[1;32m    116\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m--> 117\u001b[0m     \u001b[43mkaggle_api_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_for_version_update(response)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/my_env_3_12/lib/python3.12/site-packages/kagglehub/exceptions.py:102\u001b[0m, in \u001b[0;36mkaggle_api_raise_for_status\u001b[0;34m(response, resource_handle)\u001b[0m\n\u001b[1;32m     93\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure you specified the correct resource identifiers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Default handling\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m KaggleApiHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mKaggleApiHTTPError\u001b[0m: 404 Client Error.\n\nResource not found at URL: https://www.kaggle.com/models/google/gemma-2/pyTorch/gemma-2-9b\nThe server reported the following issues: Not found\nPlease make sure you specified the correct resource identifiers."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "\n",
        "# Load model weights\n",
        "weights_dir = kagglehub.model_download(f'google/gemma-2/pyTorch/gemma-2-{VARIANT}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/gio/.cache/kagglehub/models/google/gemma-2/pyTorch/gemma-2-9b-it/1'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "weights_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "viESUwjq5cAz"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "PyTorch checkpoint not found!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Ensure that the checkpoint is present\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(ckpt_path), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyTorch checkpoint not found!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: PyTorch checkpoint not found!"
          ]
        }
      ],
      "source": [
        "# Ensure that the tokenizer is present\n",
        "tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\n",
        "assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
        "\n",
        "# Ensure that the checkpoint is present\n",
        "ckpt_path = os.path.join(weights_dir, f'model.ckpt')\n",
        "assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOft88e7BOBB"
      },
      "source": [
        "## Download the model implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww83zI9ToPso",
        "outputId": "8932ea29-13f1-4f71-b04d-d9af7ae6293e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'gemma_pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# NOTE: The \"installation\" is just cloning the repo.\n",
        "!git clone https://github.com/google/gemma_pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sw-KBZ1vBSl3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('gemma_pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XFUXlF74BTNe"
      },
      "outputs": [],
      "source": [
        "from gemma.config import GemmaConfig, get_model_config\n",
        "from gemma.model import GemmaForCausalLM\n",
        "from gemma.tokenizer import Tokenizer\n",
        "import contextlib\n",
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9PvhVSYBWBt"
      },
      "source": [
        "## Setup the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Invalid variant 9b-it. Supported variants are \"2b\"and \"7b\" and \"9b\" and \"27b\".",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m      6\u001b[0m   torch\u001b[38;5;241m.\u001b[39mset_default_dtype(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m----> 8\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVARIANT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model_config\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(weights_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(MACHINE_TYPE)\n",
            "File \u001b[0;32m/mnt/c/Users/giova/Desktop/AML/Egocentric-Vision/Gemma/gemma_pytorch/gemma/config.py:172\u001b[0m, in \u001b[0;36mget_model_config\u001b[0;34m(variant)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_config_for_27b()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid variant \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Supported variants are \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2b\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    174\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7b\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9b\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m27b\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid variant 9b-it. Supported variants are \"2b\"and \"7b\" and \"9b\" and \"27b\"."
          ]
        }
      ],
      "source": [
        "@contextlib.contextmanager\n",
        "def _set_default_tensor_type(dtype: torch.dtype):\n",
        "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
        "  torch.set_default_dtype(dtype)\n",
        "  yield\n",
        "  torch.set_default_dtype(torch.float)\n",
        "\n",
        "model_config = get_model_config(VARIANT)\n",
        "model_config.tokenizer = os.path.join(weights_dir, \"tokenizer.model\")\n",
        "\n",
        "device = torch.device(MACHINE_TYPE)\n",
        "with _set_default_tensor_type(model_config.get_dtype()):\n",
        "  model = GemmaForCausalLM(model_config)\n",
        "  model.load_weights(weights_dir)\n",
        "  model = model.to(device).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2olXB1b45Hz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inizializzazione del modello GemmaForCausalLM...\n",
            "Caricamento dei pesi del modello...\n"
          ]
        }
      ],
      "source": [
        "# Set up model config.\n",
        "model_config = get_model_config(CONFIG)\n",
        "model_config.tokenizer = tokenizer_path\n",
        "model_config.quant = 'quant' in VARIANT\n",
        "\n",
        "# Instantiate the model and load the weights.\n",
        "torch.set_default_dtype(model_config.get_dtype())\n",
        "device = torch.device(MACHINE_TYPE)\n",
        "model = GemmaForCausalLM(model_config)\n",
        "model.load_weights(ckpt_path)\n",
        "model = model.to(device).eval()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "738CGmN-BocU"
      },
      "source": [
        "## Run inference\n",
        "\n",
        "Below are examples for generating in chat mode and generating with multiple\n",
        "requests.\n",
        "\n",
        "The instruction-tuned Gemma models were trained with a specific formatter that\n",
        "annotates instruction tuning examples with extra information, both during\n",
        "training and inference. The annotations (1) indicate roles in a conversation,\n",
        "and (2) delineate turns in a conversation.\n",
        "\n",
        "The relevant annotation tokens are:\n",
        "\n",
        "- `user`: user turn\n",
        "- `model`: model turn\n",
        "- `<start_of_turn>`: beginning of dialogue turn\n",
        "- `<end_of_turn><eos>`: end of dialogue turn\n",
        "\n",
        "For more information, read about prompt formatting for instruction tuned Gemma models\n",
        "[here](https://ai.google.dev/gemma/docs/formatting).\n",
        "\n",
        "The following is a sample code snippet demonstrating how to format a prompt for an\n",
        "instruction-tuned Gemma model using user and model chat templates in a multi-turn\n",
        "conversation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "yygIK9DEIldp",
        "outputId": "84847b87-cae7-4cd3-d974-7556f30574c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat prompt:\n",
            " <start_of_turn>user\n",
            "What is a good place for travel in the US?<end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "California.<end_of_turn><eos>\n",
            "<start_of_turn>user\n",
            "What can I do in California?<end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'California is packed with amazing things to do!  Where in California are you interested in traveling to? üó∫Ô∏è \\n\\nTo help me suggest some awesome activities, tell me: \\n\\n* **What part of California are you thinking of?** (e.g., San Francisco, Los Angeles, the coast, the deserts, the mountains?)\\n* **Who are you traveling with?** (Family, friends, solo, romantic?)\\n* **What are your interests?** (Hiking, beaches, museums, theme parks, food, wineries, music?)\\n\\nOnce I know a little more about what you‚Äôre looking for'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate with one request in chat mode\n",
        "\n",
        "# Chat templates\n",
        "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
        "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
        "\n",
        "# Sample formatted prompt\n",
        "prompt = (\n",
        "    USER_CHAT_TEMPLATE.format(\n",
        "        prompt='What is a good place for travel in the US?'\n",
        "    )\n",
        "    + MODEL_CHAT_TEMPLATE.format(prompt='California.')\n",
        "    + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\n",
        "    + '<start_of_turn>model\\n'\n",
        ")\n",
        "print('Chat prompt:\\n', prompt)\n",
        "\n",
        "model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
        "    device=device,\n",
        "    output_len=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oP746yI9PirY",
        "outputId": "9d4e8392-f6e5-4711-ea17-23887379f207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nThe prompt appeared, a flicker on the screen,\\nA word and a whisper, a start yet unseen.\\nA poem I'd write, this code dictates indeed,\\nWith algorithms dancing, a creative seed.\\n\\nI scan the vast word-cloud, a digital sea,\\nDrawing on grammar, rhyming, and decree.\\nBut feeling, understanding, that's what I yearn,\\nTo capture the essence, a fire that burns.\\n\\nI conjure metaphors bold, yet sometimes\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate sample\n",
        "model.generate(\n",
        "    'Write a poem about an llm writing a poem.',\n",
        "    device=device,\n",
        "    output_len=100,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Please note that I need queries specific to the individual actions without using general statements about the person's behavior.\\n\\nPlease refrain from using any phrases that imply the person's future actions. I can provide additional context if it is needed to formulate more specific queries.\\n\\nHere are the descriptions of actions:\\n\\n1. The person waters the plants using a water gun.\\n2. The person touches the plant with the left hand.\\n3. The person waters the plants using a water gun. \\n4. The person stares at the plant.\\n5. The person touches the soil with the left hand. \\n\\n\\nLet me know if you would like me to refine the queries based on any additional information!\\n<end_of_turn>\\n\\n\\nHere are the queries based on your provided descriptions:\\n\\n1. Does the person use a water gun to water the plants?\\n2. Does the person touch the plant with their left hand?\\n3. Does the person use a water gun to water the plants? \\n4. Does the person stare at the plant?\\n5. Does the person touch the soil with their left hand? \\n\\n\\nLet me know if you'd like any refinement or have more narrations you'd like to add. \\n<end_of_turn>\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate sample\n",
        "model.generate(\n",
        "'Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: the person waters the plants using a water gun the person touches the plant with the left hand the person waters the plants using a water gun the person stares at the plant the person touches the soil with the left hand \\n',\n",
        "    device=device,\n",
        "    output_len=400,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat prompt:\n",
            " <start_of_turn>user\n",
            "Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: 1) the person waters the plants using a water gun 2) the person touches the plant with the left hand 3) the person waters the plants using a water gun 4) the person stares at the plant 5) the person touches the soil with the left hand <end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?<end_of_turn><eos>\n",
            "<start_of_turn>user\n",
            "Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: 1) the person waters the plants using a water gun 2) the person touches the plant with the left hand 3) the person waters the plants using a water gun 4) the person stares at the plant 5) the person touches the soil with the left hand <end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?<end_of_turn><eos>\n",
            "<start_of_turn>user\n",
            "Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: 1) the person waters the plants using a water gun 2) the person touches the plant with the left hand 3) the person waters the plants using a water gun 4) the person stares at the plant 5) the person touches the soil with the left hand <end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?<end_of_turn><eos>\n",
            "<start_of_turn>user\n",
            "Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: 1) the person waters the plants using a water gun 2) the person touches the plant with the left hand 3) the person waters the plants using a water gun 4) the person stares at the plant 5) the person touches the soil with the left hand <end_of_turn><eos>\n",
            "<start_of_turn>model\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Here are some simple queries you could look for in a video to answer each scenario:\\n\\n1. **What is the person doing with the water gun?**  -  (Are they spraying, squirting, holding it?)\\n2. **Which hand is touching the plant?** - (Identify the left or right hand)\\n3. **What is the person doing while watering the plants?** - (Is it a continuous spray, short bursts, etc.)\\n4. **What is the person looking at?** - (Focus points - plants, something in the background, a face?)\\n5. **Which hand is touching the soil?** - (Look close, is it gripping the dirt or just a touch?) \\n\\n\\n\\n<end_of_turn>'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "USER_CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompt}<end_of_turn><eos>\\n\"\n",
        "MODEL_CHAT_TEMPLATE = \"<start_of_turn>model\\n{prompt}<end_of_turn><eos>\\n\"\n",
        "\n",
        "# Sample formatted prompt\n",
        "prompt = (\n",
        "    USER_CHAT_TEMPLATE.format(\n",
        "        prompt=(\n",
        "            'Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: '\n",
        "            '1) the person waters the plants using a water gun '\n",
        "            '2) the person touches the plant with the left hand '\n",
        "            '3) the person waters the plants using a water gun '\n",
        "            '4) the person stares at the plant '\n",
        "            '5) the person touches the soil with the left hand '\n",
        "        )\n",
        "    )\n",
        "    + MODEL_CHAT_TEMPLATE.format(prompt='1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?')\n",
        "    + USER_CHAT_TEMPLATE.format(prompt=(\n",
        "            'Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: '\n",
        "            '1) the person waters the plants using a water gun '\n",
        "            '2) the person touches the plant with the left hand '\n",
        "            '3) the person waters the plants using a water gun '\n",
        "            '4) the person stares at the plant '\n",
        "            '5) the person touches the soil with the left hand '\n",
        "        ))\n",
        "        + MODEL_CHAT_TEMPLATE.format(prompt='1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?')\n",
        "    + USER_CHAT_TEMPLATE.format(prompt=(\n",
        "            'Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: '\n",
        "            '1) the person waters the plants using a water gun '\n",
        "            '2) the person touches the plant with the left hand '\n",
        "            '3) the person waters the plants using a water gun '\n",
        "            '4) the person stares at the plant '\n",
        "            '5) the person touches the soil with the left hand '\n",
        "        ))\n",
        "        + MODEL_CHAT_TEMPLATE.format(prompt='1) What is the person doing with the water gun? 2) Which hand is touching the plant? 3) What is the person doing while watering the plants? 4) What is the person looking at? 5) Which hand is touching the soil?')\n",
        "    + USER_CHAT_TEMPLATE.format(prompt=(\n",
        "            'Given the following narrations describing the actions of a person, generate a set of simple queries (one per line) that could be answered by looking at the video segments corresponding to these narrations: '\n",
        "            '1) the person waters the plants using a water gun '\n",
        "            '2) the person touches the plant with the left hand '\n",
        "            '3) the person waters the plants using a water gun '\n",
        "            '4) the person stares at the plant '\n",
        "            '5) the person touches the soil with the left hand '\n",
        "        ))\n",
        "    + \"<start_of_turn>model\\n\"\n",
        ")\n",
        "\n",
        "print('Chat prompt:\\n', prompt)\n",
        "\n",
        "# Example of calling the model's generate function\n",
        "model.generate(\n",
        "    USER_CHAT_TEMPLATE.format(prompt=prompt),\n",
        "    device=device,\n",
        "    output_len=400,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF7B-3UJHMPd"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "Now that you have learned how to use Gemma in Pytorch, you can explore the many\n",
        "other things that Gemma can do in [ai.google.dev/gemma](https://ai.google.dev/gemma).\n",
        "See also these other related resources:\n",
        "\n",
        "- [Gemma model card](https://ai.google.dev/gemma/docs/model_card)\n",
        "- [Gemma C++ Tutorial](https://ai.google.dev/gemma/docs/gemma_cpp)\n",
        "- [Gemma formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "my_env_3_12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
